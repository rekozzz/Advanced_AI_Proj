<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Breast Cancer Classification Using Machine Learning</title>
  <link rel="stylesheet" href="style.css">
</head>

<body>

  <img src="img/cover.jpeg" alt="Breast Cancer Classification Project" class="cover-image">

  <h1>Breast Cancer Classification Using Machine Learning with Genetic Algorithm Feature Selection</h1>

  <div class="section">
    <h2>Project Report</h2>
    <p><strong>Date:</strong> December 5, 2025</p>
    <p><strong>Domain:</strong> Healthcare / Medical AI</p>
    <p><strong>Project Type:</strong> Comparative Machine Learning Study</p>
    <p><strong>Dataset:</strong> Wisconsin Breast Cancer Dataset</p>
  </div>

  <div class="section">
    <h2>1. Problem Statement</h2>
    <p>Breast cancer is one of the most common cancers affecting women worldwide, with early detection being critical for successful treatment and survival. Medical professionals rely on diagnostic imaging techniques such as Fine Needle Aspiration (FNA) to extract cells from suspicious breast masses for analysis. These cells are then evaluated based on various morphological characteristics to determine whether the tumor is <strong>benign (non-cancerous)</strong> or <strong>malignant (cancerous)</strong>.</p>
    
    <h3>Core Challenges:</h3>
    <ul>
      <li><strong>High Dimensionality:</strong> Medical diagnostic data contains numerous features (30 features in this dataset), many of which may be redundant, correlated, or irrelevant</li>
      <li><strong>Model Performance:</strong> Different machine learning algorithms have varying capabilities in handling medical classification tasks</li>
      <li><strong>Feature Redundancy:</strong> Using all available features can lead to increased computational cost, overfitting on training data, reduced model interpretability, and longer training and prediction times</li>
      <li><strong>Accuracy vs Efficiency Trade-off:</strong> Finding the optimal balance between model accuracy and computational efficiency</li>
    </ul>

    <h3>Research Question:</h3>
    <p><em>"Can Genetic Algorithm-based feature selection improve the performance, efficiency, and interpretability of machine learning models for breast cancer classification while maintaining or enhancing diagnostic accuracy?"</em></p>
    <p>This project addresses the need for <strong>automated, accurate, and efficient</strong> breast cancer classification systems that can assist medical professionals in making faster and more reliable diagnostic decisions.</p>
  </div>

  <div class="section new-page">
    <h2>2. Project Objectives</h2>
    
    <h3>Primary Objectives:</h3>
    <ul>
      <li><strong>Binary Classification Task:</strong> Develop machine learning models to classify breast cancer tumors as Benign (B) - Non-cancerous, not life-threatening, or Malignant (M) - Cancerous, requires immediate medical intervention</li>
      <li><strong>Comparative Algorithm Analysis:</strong> Implement and evaluate four different machine learning algorithms: Artificial Neural Network (ANN/MLP), K-Nearest Neighbors (KNN), Naive Bayes, and Support Vector Machine (SVM)</li>
      <li><strong>Feature Selection Using Genetic Algorithm:</strong> Apply evolutionary optimization to identify the most relevant feature subset, reduce dimensionality while preserving or improving classification accuracy</li>
      <li><strong>Performance Comparison:</strong> Compare each algorithm's performance WITH and WITHOUT GA-selected features</li>
    </ul>

    <h3>Secondary Objectives:</h3>
    <ul>
      <li><strong>Demonstrate Best Practices in ML Pipeline:</strong> Data preprocessing and normalization, proper train-test splitting, reproducible experiments (random_state=42), comprehensive evaluation metrics</li>
      <li><strong>Medical AI Considerations:</strong> Emphasize recall (sensitivity) as a critical metric, minimize false negatives (missing cancer cases)</li>
      <li><strong>Model Interpretability:</strong> Identify which features are most important for diagnosis, simplify models by reducing feature count</li>
    </ul>
  </div>

  <div class="section">
    <h2>3. Dataset Description</h2>
    
    <h3>Dataset Source: Wisconsin Diagnostic Breast Cancer (WDBC) Dataset</h3>
    <p>Original data contains 569 patient samples. Features computed from digitized images of Fine Needle Aspirate (FNA) of breast mass. Each sample represents measurements of cell nuclei characteristics.</p>

    <h3>Original Dataset Statistics:</h3>
    <table>
      <tr>
        <th>Attribute</th>
        <th>Value</th>
      </tr>
      <tr>
        <td>Total Samples</td>
        <td>569 patients</td>
      </tr>
      <tr>
        <td>Total Columns</td>
        <td>32 (ID + Diagnosis + 30 Features)</td>
      </tr>
      <tr>
        <td>Features</td>
        <td>30 numerical features</td>
      </tr>
      <tr>
        <td>Target Variable</td>
        <td>diagnosis (Categorical: M or B)</td>
      </tr>
      <tr>
        <td>Class Distribution</td>
        <td>Malignant (M): 212 samples (37.3%) / Benign (B): 357 samples (62.7%)</td>
      </tr>
      <tr>
        <td>Missing Values</td>
        <td>None</td>
      </tr>
      <tr>
        <td>Data Type</td>
        <td>Continuous numerical values</td>
      </tr>
    </table>

    <h3>Feature Categories (10 Base Measurements):</h3>
    <ul>
      <li><strong>Radius:</strong> Mean distance from center to perimeter points</li>
      <li><strong>Texture:</strong> Standard deviation of gray-scale values</li>
      <li><strong>Perimeter:</strong> Circumference of the cell nucleus</li>
      <li><strong>Area:</strong> Surface area of the cell nucleus</li>
      <li><strong>Smoothness:</strong> Local variation in radius lengths</li>
      <li><strong>Compactness:</strong> (perimeter² / area) - 1.0</li>
      <li><strong>Concavity:</strong> Severity of concave portions of the contour</li>
      <li><strong>Concave Points:</strong> Number of concave portions of the contour</li>
      <li><strong>Symmetry:</strong> Symmetry of the cell nucleus</li>
      <li><strong>Fractal Dimension:</strong> "Coastline approximation" - 1</li>
    </ul>
    <p><strong>Feature Groups:</strong> Mean values (10 features), Standard Error (10 features), Worst/Largest (10 features) = <strong>Total: 30 numerical features</strong></p>
  </div>

  <div class="section new-page">
    <h2>4. Dataset After Genetic Algorithm Feature Selection</h2>
    
    <table>
      <tr>
        <th>Attribute</th>
        <th>Value</th>
      </tr>
      <tr>
        <td>Total Samples</td>
        <td>569 (unchanged)</td>
      </tr>
      <tr>
        <td>Selected Features</td>
        <td>14 features (+ 1 diagnosis column)</td>
      </tr>
      <tr>
        <td>Dimensionality Reduction</td>
        <td>30 → 14 features (53.3% reduction)</td>
      </tr>
    </table>

    <h3>Selected Features (by Genetic Algorithm):</h3>
    <ul>
      <li>texture_mean</li>
      <li>area_mean</li>
      <li>compactness_mean</li>
      <li>concavity_mean</li>
      <li>symmetry_mean</li>
      <li>radius_se</li>
      <li>texture_se</li>
      <li>concavity_se</li>
      <li>radius_worst</li>
      <li>perimeter_worst</li>
      <li>smoothness_worst</li>
      <li>compactness_worst</li>
      <li>concavity_worst</li>
      <li>fractal_dimension_worst</li>
    </ul>

    <h3>Key Observations:</h3>
    <ul>
      <li>Mix of mean, standard error, and worst values</li>
      <li>Includes structural features (texture, area, compactness, concavity)</li>
      <li>Preserves shape characteristics (symmetry, fractal dimension)</li>
      <li>Includes boundary measurements (perimeter)</li>
    </ul>
  </div>

  <div class="section">
    <h2>5. Genetic Algorithm Feature Selection Process</h2>
    
    <h3>GA Configuration:</h3>
    <ul>
      <li>Population Size: 20 individuals</li>
      <li>Generations: 30</li>
      <li>Crossover Probability: 0.8 (80%)</li>
      <li>Mutation Probability: 0.1 (10%)</li>
      <li>Fitness Function: KNN Classification Accuracy</li>
      <li>Selection Method: Roulette Wheel Selection</li>
    </ul>

    <h3>How GA Works:</h3>
    <ul>
      <li><strong>Initialization:</strong> Creates 20 random binary vectors (length 30). Each bit represents whether to include (1) or exclude (0) a feature</li>
      <li><strong>Fitness Evaluation:</strong> For each individual, train a KNN classifier using selected features. Calculate accuracy on test set. Higher accuracy = better fitness</li>
      <li><strong>Selection:</strong> Roulette wheel selection favors better individuals. Probability of selection proportional to fitness</li>
      <li><strong>Crossover (80% probability):</strong> Two parent individuals exchange feature selections. Creates offspring with mixed feature combinations</li>
      <li><strong>Mutation (10% per bit):</strong> Randomly flip feature inclusion/exclusion. Introduces diversity to prevent premature convergence</li>
      <li><strong>Evolution:</strong> Repeats for 30 generations. Population improves over time. Best individual selected at the end</li>
    </ul>
  </div>

  <div class="section new-page">
    <h2>6. Dimensionality Reduction Impact Analysis</h2>
    
    <h3>Quantitative Impact:</h3>
    <table>
      <tr>
        <th>Metric</th>
        <th>Before GA</th>
        <th>After GA</th>
        <th>Change</th>
      </tr>
      <tr>
        <td>Number of Features</td>
        <td>30</td>
        <td>14</td>
        <td>-16 (-53.3%)</td>
      </tr>
      <tr>
        <td>Feature Space Size</td>
        <td>30D</td>
        <td>14D</td>
        <td>53% reduction</td>
      </tr>
      <tr>
        <td>Input Dimensionality</td>
        <td>High</td>
        <td>Medium</td>
        <td>Reduced</td>
      </tr>
      <tr>
        <td>Model Complexity</td>
        <td>Higher</td>
        <td>Lower</td>
        <td>Simplified</td>
      </tr>
    </table>

    <h3>Benefits of Dimensionality Reduction:</h3>
    <ul>
      <li><strong>Computational Efficiency:</strong> Faster Training, Reduced Memory Usage, Quicker Predictions, Lower Storage</li>
      <li><strong>Reduced Overfitting Risk:</strong> Fewer features reduce the chance of memorizing noise. Simpler models generalize better to unseen data</li>
      <li><strong>Improved Model Interpretability:</strong> 14 features are easier to understand than 30. Medical professionals can focus on key diagnostic indicators</li>
      <li><strong>Feature Relevance:</strong> Eliminates redundant features, removes noisy or irrelevant features, retains most informative features for classification</li>
      <li><strong>Curse of Dimensionality Mitigation:</strong> In high-dimensional spaces, data becomes sparse. Reduction to 14D improves algorithm effectiveness</li>
    </ul>
  </div>

  <div class="section">
    <h2>7. Performance Evaluation</h2>
    
    <h3>Evaluation Metrics:</h3>
    <ul>
      <li><strong>Accuracy:</strong> Percentage of correct predictions overall. Formula: (TP + TN) / (TP + TN + FP + FN)</li>
      <li><strong>Precision:</strong> Of all predicted malignant cases, how many are truly malignant. Formula: TP / (TP + FP)</li>
      <li><strong>Recall (Sensitivity):</strong> Of all actual malignant cases, how many were correctly detected. Formula: TP / (TP + FN). <strong>CRITICAL in medical diagnosis</strong> - missing cancer is dangerous!</li>
      <li><strong>F1 Score:</strong> Harmonic mean of precision and recall. Formula: 2 × (Precision × Recall) / (Precision + Recall)</li>
    </ul>

    <h3>Confusion Matrix:</h3>
    <ul>
      <li><strong>TN (True Negative):</strong> Correctly predicted benign</li>
      <li><strong>FP (False Positive):</strong> Predicted malignant, actually benign (false alarm)</li>
      <li><strong>FN (False Negative):</strong> Predicted benign, actually malignant ⚠️ DANGEROUS!</li>
      <li><strong>TP (True Positive):</strong> Correctly predicted malignant</li>
    </ul>
  </div>

  <div class="section new-page">
    <h2>8. Model Implementation Details</h2>
    
    <h3>1. Artificial Neural Network (MLP)</h3>
    <ul>
      <li><strong>Architecture:</strong> Input Layer: 30 neurons (no GA) or 14 neurons (with GA), Hidden Layer 1: 16 neurons (ReLU), Hidden Layer 2: 8 neurons (ReLU), Output Layer: 1 neuron (Sigmoid)</li>
      <li><strong>Training:</strong> Optimizer: Adam (lr=0.001), Loss: Binary Cross-Entropy, Epochs: 100, Batch Size: 16</li>
    </ul>

    <h3>2. K-Nearest Neighbors (KNN)</h3>
    <ul>
      <li>n_neighbors = 5</li>
      <li>Distance Metric: Euclidean</li>
      <li>Weights: Uniform</li>
      <li>Instance-based learning, no explicit training phase</li>
    </ul>

    <h3>3. Naive Bayes</h3>
    <ul>
      <li>Type: Gaussian Naive Bayes</li>
      <li>Assumes features follow Gaussian distribution</li>
      <li>Probabilistic classifier, fast training and prediction</li>
    </ul>

    <h3>4. Support Vector Machine (SVM)</h3>
    <ul>
      <li>Kernel: Radial Basis Function (RBF)</li>
      <li>Regularization Parameter (C): 1.0</li>
      <li>Gamma: 'scale'</li>
      <li>Effective in high-dimensional spaces, robust to overfitting</li>
    </ul>
  </div>

  <div class="section">
    <h2>9. Data Preprocessing</h2>
    <p>All models use consistent preprocessing:</p>
    <ul>
      <li><strong>Label Encoding:</strong> Malignant (M) → 1, Benign (B) → 0</li>
      <li><strong>Train-Test Split:</strong> Training Set: 80% (455 samples), Test Set: 20% (114 samples), Random State: 42 (for reproducibility)</li>
      <li><strong>Feature Scaling:</strong> StandardScaler applied. Transforms features to mean=0, std=1. Critical for KNN, SVM, and ANN</li>
    </ul>
  </div>

  <div class="section new-page">
    <h2>10. Performance Comparison Framework</h2>
    
    <h3>Performance Comparison Table:</h3>
    <table>
      <tr>
        <th>Model</th>
        <th>Features</th>
        <th>Accuracy</th>
        <th>Precision</th>
        <th>Recall</th>
        <th>F1 Score</th>
      </tr>
      <tr>
        <td>ANN (No GA)</td>
        <td>30</td>
        <td>?</td>
        <td>?</td>
        <td>?</td>
        <td>?</td>
      </tr>
      <tr>
        <td>ANN (With GA)</td>
        <td>14</td>
        <td>?</td>
        <td>?</td>
        <td>?</td>
        <td>?</td>
      </tr>
      <tr>
        <td>KNN (No GA)</td>
        <td>30</td>
        <td>?</td>
        <td>?</td>
        <td>?</td>
        <td>?</td>
      </tr>
      <tr>
        <td>KNN (With GA)</td>
        <td>14</td>
        <td>?</td>
        <td>?</td>
        <td>?</td>
        <td>?</td>
      </tr>
      <tr>
        <td>Naive Bayes (No GA)</td>
        <td>30</td>
        <td>?</td>
        <td>?</td>
        <td>?</td>
        <td>?</td>
      </tr>
      <tr>
        <td>Naive Bayes (With GA)</td>
        <td>14</td>
        <td>?</td>
        <td>?</td>
        <td>?</td>
        <td>?</td>
      </tr>
      <tr>
        <td>SVM (No GA)</td>
        <td>30</td>
        <td>?</td>
        <td>?</td>
        <td>?</td>
        <td>?</td>
      </tr>
      <tr>
        <td>SVM (With GA)</td>
        <td>14</td>
        <td>?</td>
        <td>?</td>
        <td>?</td>
        <td>?</td>
      </tr>
    </table>
    <p><strong>Note:</strong> To populate this table, run each model script and record the output metrics.</p>
  </div>

  <div class="section">
    <h2>11. Medical Performance Considerations</h2>
    <p>In a medical diagnostic context:</p>
    <ul>
      <li><strong>Recall (Sensitivity) > Precision:</strong> Missing cancer is life-threatening. False alarms can be verified with additional tests.</li>
      <li><strong>Target Metrics:</strong> Recall: Ideally > 95% (catch most malignant cases), Precision: > 85% (minimize unnecessary worry), F1 Score: > 90% (balanced performance)</li>
    </ul>
  </div>

  <div class="section new-page">
    <h2>12. Experimental Setup and Reproducibility</h2>
    
    <h3>Environment Requirements:</h3>
    <ul>
      <li>pandas - Data manipulation</li>
      <li>numpy - Numerical operations</li>
      <li>scikit-learn - ML algorithms and utilities</li>
      <li>tensorflow - Deep learning (ANN)</li>
    </ul>

    <h3>Execution Workflow:</h3>
    <ul>
      <li><strong>Step 1 - Feature Selection:</strong> python src/genetic_algorithm.py → Output: breast_cancer_selected_features.csv</li>
      <li><strong>Step 2 - Models WITHOUT GA:</strong> python src/ann_no_GA.py, python src/knn_no_GA.py, python src/naive_bayes_no_GA.py, python src/support_vector_machine_no_GA.py</li>
      <li><strong>Step 3 - Models WITH GA:</strong> python src/ann_GA.py, python src/knn_GA.py, python src/naive_bayes_GA.py, python src/support_vector_machine_GA.py</li>
    </ul>
  </div>

  <div class="section">
    <h2>13. Project Structure</h2>
    <ul>
      <li><strong>src/data_set/</strong> - data.csv (Original 569×32), breast_cancer_selected_features.csv (GA-selected 569×15)</li>
      <li><strong>src/genetic_algorithm.py</strong> - GA feature selection</li>
      <li><strong>src/processing.py</strong> - Data exploration (original)</li>
      <li><strong>src/ann_GA.py, ann_no_GA.py</strong> - ANN models</li>
      <li><strong>src/knn_GA.py, knn_no_GA.py</strong> - KNN models</li>
      <li><strong>src/naive_bayes_GA.py, naive_bayes_no_GA.py</strong> - Naive Bayes models</li>
      <li><strong>src/support_vector_machine_GA.py, support_vector_machine_no_GA.py</strong> - SVM models</li>
    </ul>
  </div>

  <div class="section new-page">
    <h2>14. Key Findings and Expected Outcomes</h2>
    
    <h3>Hypotheses:</h3>
    <ul>
      <li><strong>Hypothesis 1:</strong> GA will select 10-15 most relevant features → ✅ Confirmed: 14 features selected (53% reduction)</li>
      <li><strong>Hypothesis 2:</strong> GA models will train faster → ⏳ To be verified</li>
      <li><strong>Hypothesis 3:</strong> GA models will maintain or improve accuracy → ⏳ To be verified</li>
      <li><strong>Hypothesis 4:</strong> Different algorithms will have different optimal feature sets</li>
    </ul>

    <h3>Expected Results:</h3>
    <ul>
      <li><strong>KNN:</strong> Expected to benefit most from dimensionality reduction (curse of dimensionality affects distance-based algorithms)</li>
      <li><strong>SVM:</strong> Should perform well with or without GA (already handles high dimensions effectively)</li>
      <li><strong>ANN:</strong> May have slight accuracy trade-off but faster training expected with 14 features</li>
      <li><strong>Naive Bayes:</strong> Variable performance - GA may help by removing redundant features</li>
    </ul>
  </div>

  <div class="section">
    <h2>15. Limitations and Future Work</h2>
    
    <h3>Current Limitations:</h3>
    <ul>
      <li>Single Feature Set: GA optimization performed once using KNN as fitness function</li>
      <li>Fixed GA Parameters: No hyperparameter tuning performed</li>
      <li>Single Train-Test Split: More robust evaluation would use cross-validation</li>
      <li>Limited Fitness Functions: GA uses only accuracy</li>
      <li>No Ensemble Methods: Individual classifiers only</li>
    </ul>

    <h3>Future Enhancements:</h3>
    <ul>
      <li>Algorithm-Specific Feature Selection</li>
      <li>Multi-Objective Optimization</li>
      <li>Cross-Validation (5-fold or 10-fold)</li>
      <li>Hyperparameter Tuning</li>
      <li>Advanced Feature Selection Methods (RFE, LASSO, PCA)</li>
      <li>Ensemble Methods</li>
      <li>Explainable AI (SHAP, LIME)</li>
      <li>Deployment Considerations (REST API, Web interface)</li>
    </ul>
  </div>

  <div class="section new-page">
    <h2>16. Conclusions</h2>
    
    <h3>Project Accomplishments:</h3>
    <ul>
      <li>✅ <strong>Complete ML Pipeline:</strong> From raw data to trained models</li>
      <li>✅ <strong>Evolutionary Optimization:</strong> GA for intelligent feature selection</li>
      <li>✅ <strong>Comparative Analysis:</strong> 8 different model configurations (4 algorithms × 2 versions)</li>
      <li>✅ <strong>Dimensionality Reduction:</strong> 53% feature reduction (30 → 14)</li>
      <li>✅ <strong>Reproducible Science:</strong> Fixed random seeds, documented methodology</li>
      <li>✅ <strong>Medical AI Context:</strong> Appropriate metrics and considerations</li>
    </ul>

    <h3>Impact of Genetic Algorithm:</h3>
    <ul>
      <li><strong>Efficiency:</strong> Reduced feature space by 53%</li>
      <li><strong>Automation:</strong> No manual feature engineering required</li>
      <li><strong>Objectivity:</strong> Data-driven, not based on human bias</li>
      <li><strong>Optimization:</strong> Explores hundreds of feature combinations</li>
    </ul>

    <h3>Practical Applications:</h3>
    <ul>
      <li>Clinical Decision Support: Faster, automated cancer screening</li>
      <li>Medical Imaging: Identifying key diagnostic features</li>
      <li>Healthcare AI: Template for other diagnostic tasks</li>
      <li>Feature Engineering: Demonstrating GA effectiveness</li>
    </ul>
  </div>

  <div class="section">
    <h2>17. References and Resources</h2>
    
    <h3>Dataset:</h3>
    <p><strong>Wisconsin Diagnostic Breast Cancer Dataset</strong> - UCI Machine Learning Repository. Creators: Dr. William H. Wolberg, W. Nick Street, Olvi L. Mangasarian</p>

    <h3>Algorithms:</h3>
    <ul>
      <li>K-Nearest Neighbors: Cover & Hart (1967)</li>
      <li>Naive Bayes: Based on Bayes' Theorem</li>
      <li>Support Vector Machines: Cortes & Vapnik (1995)</li>
      <li>Artificial Neural Networks: Backpropagation (Rumelhart et al., 1986)</li>
      <li>Genetic Algorithms: Holland (1975)</li>
    </ul>

    <h3>Libraries:</h3>
    <ul>
      <li>scikit-learn: Pedregosa et al. (2011)</li>
      <li>TensorFlow/Keras: Abadi et al. (2015)</li>
      <li>pandas: McKinney (2010)</li>
      <li>NumPy: Harris et al. (2020)</li>
    </ul>
  </div>

  <div class="section">
    <div class="section">
      <h2>Summary</h2>
      <p>This project successfully demonstrates a complete machine learning pipeline for breast cancer classification using the Wisconsin Diagnostic Breast Cancer Dataset. The implementation compares four different algorithms (ANN, KNN, Naive Bayes, SVM) with and without Genetic Algorithm-based feature selection.</p>

      <p>Key achievements include:</p>
      <ul>
        <li>53% dimensionality reduction (30 → 14 features) through GA optimization</li>
        <li>Comprehensive evaluation using accuracy, precision, recall, and F1 score metrics</li>
        <li>Emphasis on recall (sensitivity) for medical AI applications</li>
        <li>Reproducible experiments with documented methodology</li>
      </ul>

      <p><strong>Report Prepared By:</strong> Advanced AI Project Team</p>
      <p><strong>Last Updated:</strong> December 5, 2025</p>
      <p><strong>Project Status:</strong> Completed - Ready for Experimentation</p>
      <p><strong>License:</strong> Educational Use Only</p>

      <p><em>⚠️ Important Disclaimer: This project is for research and educational purposes only. NOT approved for clinical use and NOT a substitute for professional medical diagnosis.</em></p>
    </div>
</body>

</html>
